import json
import os
import re
import traceback
from typing import Dict, Any

from dotenv import load_dotenv
from openai import OpenAI

from util.cache.ai_cache import AICache
from util.log.log import Log
from documents_multi_agents.domain.service.hybrid_parser import HybridParser

load_dotenv()
logger = Log.get_logger()
log_util = Log()


class FinancialAnalyzerService:
    """
    Redisì—ì„œ ë³µí˜¸í™”ëœ ì¬ë¬´ ë°ì´í„°ë¥¼ AIë¡œ ë¶„ì„í•˜ê³  ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•˜ëŠ” ì„œë¹„ìŠ¤
    """

    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    @staticmethod
    def _fix_json_string(json_str: str) -> str:
        """
        ì˜ëª»ëœ JSON ë¬¸ìì—´ì„ ìˆ˜ì •
        """
        # 1. ë§ˆì§€ë§‰ í•­ëª©ì˜ ì‰¼í‘œ ì œê±°
        json_str = re.sub(r',\s*}', '}', json_str)
        json_str = re.sub(r',\s*]', ']', json_str)

        # 2. ì—°ì†ëœ ì‰¼í‘œ ì œê±°
        json_str = re.sub(r',\s*,', ',', json_str)

        # 3. ì½œë¡  ë’¤ì— ì‰¼í‘œê°€ ë°”ë¡œ ì˜¤ëŠ” ê²½ìš° ìˆ˜ì •
        json_str = re.sub(r':\s*,', ': null,', json_str)

        return json_str

    @staticmethod
    def _clean_item_names(data: Dict) -> Dict:
        """
        í•­ëª©ëª…ì˜ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ë„ì–´ì“°ê¸°ë¡œ ë³€í™˜í•˜ê³  ë°ì´í„° ì •ë¦¬
        """
        if not isinstance(data, dict):
            return data

        cleaned = {}
        for key, value in data.items():
            # í‚¤ì˜ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ë„ì–´ì“°ê¸°ë¡œ ë³€í™˜
            clean_key = key.replace("_", " ")

            if isinstance(value, dict):
                # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ë„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                cleaned[clean_key] = FinancialAnalyzerService._clean_item_names(value)
            else:
                cleaned[clean_key] = value

        return cleaned

    @log_util.logging_decorator
    def categorize_financial_data(self, decrypted_data: Dict[str, str]) -> Dict[str, Any]:
        """
        ë³µí˜¸í™”ëœ ì¬ë¬´ ë°ì´í„°ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
        
        Args:
            decrypted_data: ë³µí˜¸í™”ëœ ë°ì´í„° {"ì†Œë“:ê¸‰ì—¬": "3000000", "ì§€ì¶œ:ì‹ë¹„": "500000", ...}
            
        Returns:
            ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ëœ ë°ì´í„°
        """
        # ì†Œë“/ì§€ì¶œ ë¶„ë¦¬
        income_items = {}
        expense_items = {}

        for key, value in decrypted_data.items():
            if key == "USER_TOKEN":
                continue

            # "íƒ€ì…:í•­ëª©" í˜•íƒœì—ì„œ ë¶„ë¦¬
            if ":" in key:
                doc_type, field = key.split(":", 1)

                if "ì†Œë“" in doc_type or "income" in doc_type.lower():
                    income_items[field] = value
                elif "ì§€ì¶œ" in doc_type or "expense" in doc_type.lower():
                    expense_items[field] = value

        # AIë¡œ ê°ê° ë¶„ì„
        categorized_income = self._categorize_income(income_items) if income_items else {}
        categorized_expense = self._categorize_expense(expense_items) if expense_items else {}

        # ì¢…í•© ë¶„ì„ ë° ì¶”ì²œ
        recommendations = self._generate_recommendations(categorized_income, categorized_expense)

        return {
            "income": categorized_income,
            "expense": categorized_expense,
            "recommendations": recommendations,
            "summary": self._generate_summary(categorized_income, categorized_expense)
        }

    @log_util.logging_decorator
    def _categorize_income(self, income_items: Dict[str, str]) -> Dict[str, Any]:
        """ì†Œë“ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        if not income_items:
            return {}

        # ğŸ”¥ ìºì‹œ í‚¤ ìƒì„± (ë°ì´í„° ê¸°ë°˜)
        data_str = json.dumps(income_items, ensure_ascii=False, sort_keys=True)
        cache_key = AICache.generate_cache_key(data_str, "categorize-income")
        
        # ğŸ”¥ ìºì‹œ í™•ì¸
        cached_response = AICache.get_cached_response(cache_key)
        if cached_response:
            try:
                logger.info("[CACHE HIT] ì†Œë“ ë¶„ë¥˜ ìºì‹œ ì‚¬ìš©")
                return json.loads(cached_response)
            except json.JSONDecodeError:
                logger.warning("[CACHE] Failed to parse cached income data, re-analyzing")

        # ============================================
        # ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± (ê·œì¹™ ê¸°ë°˜ ìš°ì„ )
        # ============================================
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ“Š [HYBRID PARSING START] ì†Œë“ í•­ëª© ë¶„ë¥˜ ì‹œì‘ ({len(income_items)}ê°œ í•­ëª©)")
        logger.info(f"{'='*80}")
        
        try:
            hybrid_parser = HybridParser()  # âœï¸ confidence_threshold ì œê±°
        except Exception as e:
            logger.error(f"âŒ [HYBRID PARSER] ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            logger.error(f"   ìŠ¤íƒ: {traceback.format_exc()}")
            # í´ë°±: GPTë§Œ ì‚¬ìš©
            hybrid_parser = None
        
        confident_items = {}  # ê·œì¹™ ê¸°ë°˜ ì„±ê³µ
        uncertain_items = {}  # GPT í•„ìš”
        
        if hybrid_parser:
            for field_name, value in income_items.items():
                try:
                    trans_type, category, metadata = hybrid_parser.classify_item(
                        field_name, 
                        value, 
                        doc_type_hint='ì†Œë“'
                    )
                    
                    if metadata['method'] == 'rule_based':
                        # âœ… ê·œì¹™ ê¸°ë°˜ ì„±ê³µ
                        confident_items[field_name] = value
                    else:
                        # âš ï¸ GPT í•„ìš”
                        uncertain_items[field_name] = value
                except Exception as e:
                    logger.warning(f"âš ï¸  [PARSE ERROR] '{field_name}' íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    uncertain_items[field_name] = value
            
            # ğŸ“Š í†µê³„ ì¶œë ¥
            try:
                stats = hybrid_parser.get_statistics()
                logger.info(f"\nğŸ“Š [PARSING STATS]")
                logger.info(f"   ì „ì²´: {stats['total_items']}ê°œ")
                logger.info(f"   âœ… DB ê·œì¹™ ì„±ê³µ: {stats['db_rule_success']}ê°œ ({stats['db_rule_rate']*100:.1f}%)")
                logger.info(f"   âš ï¸  GPT í•„ìš”: {stats['gpt_fallback']}ê°œ ({stats['gpt_fallback_rate']*100:.1f}%)")
                logger.info(f"   ğŸ’° ë¹„ìš© ì ˆê°ë¥ : {stats['cost_saving_rate']*100:.1f}%")
                logger.info(f"   ğŸ“ ìƒˆë¡œ í•™ìŠµ: {stats['new_keywords_learned']}ê°œ")
            except Exception as e:
                logger.warning(f"âš ï¸  í†µê³„ ì¶œë ¥ ì‹¤íŒ¨: {str(e)}")
        else:
            # HybridParser ì‹¤íŒ¨ ì‹œ ëª¨ë“  í•­ëª©ì„ GPTë¡œ
            logger.warning("âš ï¸  HybridParserë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  í•­ëª©ì„ GPTë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            uncertain_items = income_items.copy()
        
        # ============================================
        # GPTë¡œ ì „ì²´ ì¬ë¶„ì„ (ë¶ˆí™•ì‹¤í•œ í•­ëª© í¬í•¨)
        # ============================================
        if uncertain_items:
            logger.warning(f"\nâš ï¸  [{len(uncertain_items)}ê°œ í•­ëª©] GPTë¡œ ì¬ë¶„ì„ í•„ìš”:")
            for field in uncertain_items.keys():
                logger.warning(f"   - {field}")
            logger.info(f"\nğŸ¤– [GPT PARSING] ì „ì²´ í•­ëª©ì„ GPTë¡œ ë¶„ì„í•©ë‹ˆë‹¤...")
        else:
            logger.info(f"\nâœ… [100% DB-RULE] ëª¨ë“  í•­ëª©ì„ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤!")
            logger.info(f"ğŸ¤– [GPT PARSING] ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ GPTë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")

        # ê¸°ì¡´ GPT í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì „ì²´ í•­ëª©)
        prompt = f"""
ë‹¤ìŒ ì†Œë“ í•­ëª©ë“¤ì„ ë¶„ì„í•˜ì—¬ ì•„ë˜ ì¹´í…Œê³ ë¦¬ë¡œ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•´ì¤˜:

ì†Œë“ í•­ëª©:
{json.dumps(income_items, ensure_ascii=False, indent=2)}

**ì—„ê²©í•œ ë¶„ë¥˜ ê¸°ì¤€:**

1. ê³ ì •ì†Œë“: ë§¤ì›” ì¼ì •í•˜ê²Œ ë“¤ì–´ì˜¤ëŠ” ì†Œë“
   - ê¸‰ì—¬, ì›”ê¸‰, ì—°ë´‰
   - ì‹ëŒ€ (ê³ ì •)
   - ì •ê¸° ìˆ˜ë‹¹

2. ë³€ë™ì†Œë“: ë¶ˆê·œì¹™ì ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ì†Œë“
   - ìƒì—¬ê¸ˆ, ë³´ë„ˆìŠ¤, ì„±ê³¼ê¸‰
   - ìˆ˜ë‹¹ (ë³€ë™)
   - ì•¼ê·¼ìˆ˜ë‹¹, ì—°ì¥ê·¼ë¡œìˆ˜ë‹¹

3. ê¸°íƒ€ì†Œë“: ë¶€ê°€ ìˆ˜ì…
   - ì´ìì†Œë“, ë°°ë‹¹ì†Œë“
   - ì„ëŒ€ì†Œë“
   - í”„ë¦¬ëœì„œ ìˆ˜ì…

**ì ˆëŒ€ ê·œì¹™:**
1. í•­ëª©ëª…ì˜ ì–¸ë”ìŠ¤ì½”ì–´(_)ë¥¼ ë„ì–´ì“°ê¸°ë¡œ ë³€ê²½
2. ì›ë³¸ ê¸ˆì•¡ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìˆ«ì íƒ€ì…)
3. ë¹ˆ ê°ì²´ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
4. JSON ë¬¸ë²• ì—„ìˆ˜: ë§ˆì§€ë§‰ í•­ëª© ë’¤ì— ì‰¼í‘œ ì—†ìŒ, ëª¨ë“  ê´„í˜¸ ì •í™•íˆ ë‹«ê¸°

**ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥¼ ê²ƒ):**

```json
{{
  "ê³ ì •ì†Œë“": {{
    "ê¸‰ì—¬": 3000000,
    "ì‹ëŒ€": 200000
  }},
  "ë³€ë™ì†Œë“": {{
    "ìƒì—¬": 1000000
  }},
  "ê¸°íƒ€ì†Œë“": {{
    "ì´ì": 50000
  }},
  "ì¹´í…Œê³ ë¦¬ë³„ í•©ê³„": {{
    "ê³ ì •ì†Œë“": 3200000,
    "ë³€ë™ì†Œë“": 1000000,
    "ê¸°íƒ€ì†Œë“": 50000
  }},
  "ì´ì†Œë“": 4250000
}}
```

ì¤‘ìš”: ìœ„ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. JSON ì½”ë“œë¸”ë¡(```)ì€ ì œì™¸í•˜ê³  ìˆœìˆ˜ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0,
                seed=12345
            )

            result_text = response.choices[0].message.content.strip()

            # JSON ì¶”ì¶œ
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()

            # JSON ìˆ˜ì • (ì˜ëª»ëœ ë¬¸ë²• ìë™ ìˆ˜ì •)
            result_text = self._fix_json_string(result_text)

            # JSON íŒŒì‹± ì‹œë„
            try:
                result = json.loads(result_text)
                # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ë„ì–´ì“°ê¸°ë¡œ ë³€í™˜
                cleaned_result = self._clean_item_names(result)
                
                # ğŸ“ GPT í•™ìŠµ: ë¶ˆí™•ì‹¤í–ˆë˜ í•­ëª©ë“¤ì„ DBì— ì €ì¥
                if uncertain_items:
                    self._learn_from_gpt_income(uncertain_items, cleaned_result)
                
                # âœ… GPT ë¶„ì„ ì™„ë£Œ ë¡œê¹…
                logger.info(f"\nâœ… [GPT COMPLETED] ì†Œë“ ë¶„ë¥˜ ì™„ë£Œ")
                logger.info(f"{'='*80}\n")
                
                # ğŸ”¥ ìºì‹œ ì €ì¥ (24ì‹œê°„)
                AICache.set_cached_response(cache_key, json.dumps(cleaned_result, ensure_ascii=False), ttl=86400)
                
                return cleaned_result
            except json.JSONDecodeError as json_err:
                logger.error(f"[ERROR] JSON parsing failed: {json_err}")
                logger.error(f"[ERROR] Raw response text: {result_text}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°ì´í„° ë°˜í™˜
                return {
                    "error": f"AI ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(json_err)}",
                    "raw_items": income_items,
                    "ê³ ì •ì†Œë“": {},
                    "ë³€ë™ì†Œë“": {},
                    "ê¸°íƒ€ì†Œë“": {},
                    "ì¹´í…Œê³ ë¦¬ë³„ í•©ê³„": {
                        "ê³ ì •ì†Œë“": 0,
                        "ë³€ë™ì†Œë“": 0,
                        "ê¸°íƒ€ì†Œë“": 0
                    },
                    "ì´ì†Œë“": sum(int(v) for v in income_items.values() if v.isdigit())
                }
        except Exception as e:
            logger.error(f"[ERROR] Income categorization failed: {str(e)}")
            return {
                "error": str(e),
                "raw_items": income_items,
                "ê³ ì •ì†Œë“": {},
                "ë³€ë™ì†Œë“": {},
                "ê¸°íƒ€ì†Œë“": {},
                "ì¹´í…Œê³ ë¦¬ë³„ í•©ê³„": {
                    "ê³ ì •ì†Œë“": 0,
                    "ë³€ë™ì†Œë“": 0,
                    "ê¸°íƒ€ì†Œë“": 0
                },
                "ì´ì†Œë“": sum(int(v) for v in income_items.values() if v.isdigit())
            }

    @log_util.logging_decorator
    def _categorize_expense(self, expense_items: Dict[str, str]) -> Dict[str, Any]:
        """ì§€ì¶œì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        if not expense_items:
            return {}

        # ğŸ”¥ ìºì‹œ í‚¤ ìƒì„± (ë°ì´í„° ê¸°ë°˜)
        data_str = json.dumps(expense_items, ensure_ascii=False, sort_keys=True)
        cache_key = AICache.generate_cache_key(data_str, "categorize-expense")
        
        # ğŸ”¥ ìºì‹œ í™•ì¸
        cached_response = AICache.get_cached_response(cache_key)
        if cached_response:
            try:
                logger.info("[CACHE HIT] ì§€ì¶œ ë¶„ë¥˜ ìºì‹œ ì‚¬ìš©")
                return json.loads(cached_response)
            except json.JSONDecodeError:
                logger.warning("[CACHE] Failed to parse cached expense data, re-analyzing")

        # ============================================
        # ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± (ê·œì¹™ ê¸°ë°˜ ìš°ì„ )
        # ============================================
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ“Š [HYBRID PARSING START] ì§€ì¶œ í•­ëª© ë¶„ë¥˜ ì‹œì‘ ({len(expense_items)}ê°œ í•­ëª©)")
        logger.info(f"{'='*80}")
        
        try:
            hybrid_parser = HybridParser()  # âœï¸ confidence_threshold ì œê±°
        except Exception as e:
            logger.error(f"âŒ [HYBRID PARSER] ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            logger.error(f"   ìŠ¤íƒ: {traceback.format_exc()}")
            # í´ë°±: GPTë§Œ ì‚¬ìš©
            hybrid_parser = None
        
        confident_items = {}  # ê·œì¹™ ê¸°ë°˜ ì„±ê³µ
        uncertain_items = {}  # GPT í•„ìš”
        
        if hybrid_parser:
            for field_name, value in expense_items.items():
                try:
                    trans_type, category, metadata = hybrid_parser.classify_item(
                        field_name, 
                        value, 
                        doc_type_hint='ì§€ì¶œ'
                    )
                    
                    if metadata['method'] == 'rule_based':
                        # âœ… ê·œì¹™ ê¸°ë°˜ ì„±ê³µ
                        confident_items[field_name] = value
                    else:
                        # âš ï¸ GPT í•„ìš”
                        uncertain_items[field_name] = value
                except Exception as e:
                    logger.warning(f"âš ï¸  [PARSE ERROR] '{field_name}' íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    uncertain_items[field_name] = value
            
            # ğŸ“Š í†µê³„ ì¶œë ¥
            try:
                stats = hybrid_parser.get_statistics()
                logger.info(f"\nğŸ“Š [PARSING STATS]")
                logger.info(f"   ì „ì²´: {stats['total_items']}ê°œ")
                logger.info(f"   âœ… DB ê·œì¹™ ì„±ê³µ: {stats['db_rule_success']}ê°œ ({stats['db_rule_rate']*100:.1f}%)")
                logger.info(f"   âš ï¸  GPT í•„ìš”: {stats['gpt_fallback']}ê°œ ({stats['gpt_fallback_rate']*100:.1f}%)")
                logger.info(f"   ğŸ’° ë¹„ìš© ì ˆê°ë¥ : {stats['cost_saving_rate']*100:.1f}%")
                logger.info(f"   ğŸ“ ìƒˆë¡œ í•™ìŠµ: {stats['new_keywords_learned']}ê°œ")
            except Exception as e:
                logger.warning(f"âš ï¸  í†µê³„ ì¶œë ¥ ì‹¤íŒ¨: {str(e)}")
        else:
            # HybridParser ì‹¤íŒ¨ ì‹œ ëª¨ë“  í•­ëª©ì„ GPTë¡œ
            logger.warning("âš ï¸  HybridParserë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  í•­ëª©ì„ GPTë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            uncertain_items = expense_items.copy()
        
        # ============================================
        # GPTë¡œ ì „ì²´ ì¬ë¶„ì„ (ë¶ˆí™•ì‹¤í•œ í•­ëª© í¬í•¨)
        # ============================================
        if uncertain_items:
            logger.warning(f"\nâš ï¸  [{len(uncertain_items)}ê°œ í•­ëª©] GPTë¡œ ì¬ë¶„ì„ í•„ìš”:")
            for field in uncertain_items.keys():
                logger.warning(f"   - {field}")
            logger.info(f"\nğŸ¤– [GPT PARSING] ì „ì²´ í•­ëª©ì„ GPTë¡œ ë¶„ì„í•©ë‹ˆë‹¤...")
        else:
            logger.info(f"\nâœ… [100% DB-RULE] ëª¨ë“  í•­ëª©ì„ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤!")
            logger.info(f"ğŸ¤– [GPT PARSING] ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ GPTë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")


        # ê¸°ì¡´ GPT í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        prompt = f"""
ë‹¤ìŒ ì§€ì¶œ í•­ëª©ë“¤ì„ ë¶„ì„í•˜ì—¬ ì•„ë˜ ì¹´í…Œê³ ë¦¬ë¡œ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•´ì¤˜:

ì§€ì¶œ í•­ëª©:
{json.dumps(expense_items, ensure_ascii=False, indent=2)}

**ì—„ê²©í•œ ë¶„ë¥˜ ê¸°ì¤€:**

1. ê³ ì •ì§€ì¶œ (ë§¤ë‹¬ ì¼ì •í•˜ê²Œ ë‚˜ê°€ëŠ” ê³ ì • ê¸ˆì•¡):
   - ì›”ì„¸, ê´€ë¦¬ë¹„, ì£¼íƒë‹´ë³´ëŒ€ì¶œ
   - í†µì‹ ë¹„ (íœ´ëŒ€í°, ì¸í„°ë„·, TV)
   - ë³´í—˜ë£Œ (ê±´ê°•ë³´í—˜, ìë™ì°¨ë³´í—˜, ìƒëª…ë³´í—˜, ì‹¤ì†ë³´í—˜ ë“± ëª¨ë“  ë³´í—˜)
   - êµ¬ë…ë£Œ (ë„·í”Œë¦­ìŠ¤, ë©œë¡  ë“±)
   - êµí†µë¹„ ì •ê¸°ê¶Œ
   - í•™ì›ë¹„, ë“±ë¡ê¸ˆ (ì •ê¸° ë‚©ë¶€)
   
2. ë³€ë™ì§€ì¶œ (ë§¤ë‹¬ ê¸ˆì•¡ì´ ë‹¬ë¼ì§€ëŠ” ì§€ì¶œ):
   - ì‹ë¹„, ì™¸ì‹ë¹„, ë°°ë‹¬ìŒì‹
   - ì‡¼í•‘ (ì˜ë¥˜, ì¡í™”, í™”ì¥í’ˆ)
   - ë¬¸í™”ìƒí™œ (ì˜í™”, ê³µì—°, ì·¨ë¯¸)
   - êµí†µë¹„ (íƒì‹œ, ì£¼ìœ ë¹„, ëŒ€ì¤‘êµí†µ)
   - ì˜ë£Œë¹„
   - ì¹´ë“œ ì‚¬ìš©ì•¡ (ì „í†µì‹œì¥, ì¼ë°˜ ì¹´ë“œ ì‚¬ìš©)
   
3. ì €ì¶• ë° íˆ¬ì:
   - ì ê¸ˆ, ì˜ˆê¸ˆ, ì²­ì•½ì €ì¶•
   - ì£¼ì‹, í€ë“œ, ì±„ê¶Œ
   - ì—°ê¸ˆì €ì¶•
   - ëŒ€ì¶œ ì›ê¸ˆ ìƒí™˜
   
4. ê¸°íƒ€ ë° ì˜ˆë¹„ë¹„ (ì¼íšŒì„± ë˜ëŠ” ë¶„ë¥˜ ì• ë§¤í•œ ì§€ì¶œ):
   - ë³‘ì›ë¹„ (í° ì¹˜ë£Œë¹„)
   - ê²½ì¡°ì‚¬ë¹„
   - ì„ ë¬¼ë¹„
   - ìˆ˜ë¦¬ë¹„
   - ì¼íšŒì„± ì§€ì¶œ

**ì ˆëŒ€ ê·œì¹™:**
1. ëª¨ë“  ë³´í—˜ë£ŒëŠ” ë°˜ë“œì‹œ "ê³ ì •ì§€ì¶œ"ì— í¬í•¨
2. ì¹´ë“œ ì‚¬ìš©ì•¡ì€ "ë³€ë™ì§€ì¶œ"ì— í¬í•¨
3. í•­ëª©ëª…ì˜ ì–¸ë”ìŠ¤ì½”ì–´(_)ë¥¼ ë„ì–´ì“°ê¸°ë¡œ ë³€ê²½
4. ì›ë³¸ ê¸ˆì•¡ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìˆ«ì íƒ€ì…)
5. ë¹ˆ ê°ì²´ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
6. JSON ë¬¸ë²• ì—„ìˆ˜: ë§ˆì§€ë§‰ í•­ëª© ë’¤ì— ì‰¼í‘œ ì—†ìŒ, ëª¨ë“  ê´„í˜¸ ì •í™•íˆ ë‹«ê¸°

**ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥¼ ê²ƒ):**

```json
{{
  "ê³ ì •ì§€ì¶œ": {{
    "ì›”ì„¸": 1000000,
    "êµ­ë¯¼ì—°ê¸ˆë³´í—˜ë£Œ ì´í•©ê³„": 675000
  }},
  "ë³€ë™ì§€ì¶œ": {{
    "ì‹ë¹„": 300000,
    "ì¹´ë“œ ì „í†µì‹œì¥ í•©ê³„": 120000
  }},
  "ì €ì¶• ë° íˆ¬ì": {{
    "ì ê¸ˆ": 500000
  }},
  "ê¸°íƒ€ ë° ì˜ˆë¹„ë¹„": {{
    "ê²½ì¡°ì‚¬ë¹„": 100000
  }},
  "ì¹´í…Œê³ ë¦¬ë³„ í•©ê³„": {{
    "ê³ ì •ì§€ì¶œ": 1675000,
    "ë³€ë™ì§€ì¶œ": 420000,
    "ì €ì¶• ë° íˆ¬ì": 500000,
    "ê¸°íƒ€ ë° ì˜ˆë¹„ë¹„": 100000
  }},
  "ì´ì§€ì¶œ": 2695000
}}
```

ì¤‘ìš”: ìœ„ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. JSON ì½”ë“œë¸”ë¡(```)ì€ ì œì™¸í•˜ê³  ìˆœìˆ˜ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0,
                seed=12345
            )

            result_text = response.choices[0].message.content.strip()

            # JSON ì¶”ì¶œ
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()

            # JSON ìˆ˜ì • (ì˜ëª»ëœ ë¬¸ë²• ìë™ ìˆ˜ì •)
            result_text = self._fix_json_string(result_text)

            # JSON íŒŒì‹± ì‹œë„
            try:
                result = json.loads(result_text)
                # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ë„ì–´ì“°ê¸°ë¡œ ë³€í™˜
                cleaned_result = self._clean_item_names(result)
                
                # ğŸ“ GPT í•™ìŠµ: ë¶ˆí™•ì‹¤í–ˆë˜ í•­ëª©ë“¤ì„ DBì— ì €ì¥
                if uncertain_items:
                    self._learn_from_gpt_expense(uncertain_items, cleaned_result)
                
                # âœ… GPT ë¶„ì„ ì™„ë£Œ ë¡œê¹…
                logger.info(f"\nâœ… [GPT COMPLETED] ì§€ì¶œ ë¶„ë¥˜ ì™„ë£Œ")
                logger.info(f"{'='*80}\n")
                
                # ğŸ”¥ ìºì‹œ ì €ì¥ (24ì‹œê°„)
                AICache.set_cached_response(cache_key, json.dumps(cleaned_result, ensure_ascii=False), ttl=86400)
                
                return cleaned_result
            except json.JSONDecodeError as json_err:
                logger.error(f"[ERROR] JSON parsing failed: {json_err}")
                logger.error(f"[ERROR] Raw response text: {result_text}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°ì´í„° ë°˜í™˜
                return {
                    "error": f"AI ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(json_err)}",
                    "raw_items": expense_items,
                    "ê³ ì •ì§€ì¶œ": {},
                    "ë³€ë™ì§€ì¶œ": {},
                    "ì €ì¶• ë° íˆ¬ì": {},
                    "ê¸°íƒ€ ë° ì˜ˆë¹„ë¹„": {},
                    "ì¹´í…Œê³ ë¦¬ë³„ í•©ê³„": {
                        "ê³ ì •ì§€ì¶œ": 0,
                        "ë³€ë™ì§€ì¶œ": 0,
                        "ì €ì¶• ë° íˆ¬ì": 0,
                        "ê¸°íƒ€ ë° ì˜ˆë¹„ë¹„": 0
                    },
                    "ì´ì§€ì¶œ": sum(int(v) for v in expense_items.values() if v.isdigit())
                }
        except Exception as e:
            logger.error(f"[ERROR] Expense categorization failed: {str(e)}")
            return {
                "error": str(e),
                "raw_items": expense_items,
                "ê³ ì •ì§€ì¶œ": {},
                "ë³€ë™ì§€ì¶œ": {},
                "ì €ì¶• ë° íˆ¬ì": {},
                "ê¸°íƒ€ ë° ì˜ˆë¹„ë¹„": {},
                "ì¹´í…Œê³ ë¦¬ë³„ í•©ê³„": {
                    "ê³ ì •ì§€ì¶œ": 0,
                    "ë³€ë™ì§€ì¶œ": 0,
                    "ì €ì¶• ë° íˆ¬ì": 0,
                    "ê¸°íƒ€ ë° ì˜ˆë¹„ë¹„": 0
                },
                "ì´ì§€ì¶œ": sum(int(v) for v in expense_items.values() if v.isdigit())
            }

    @log_util.logging_decorator
    def _generate_recommendations(self, income_data: Dict, expense_data: Dict, use_ai: bool = False) -> Dict[str, Any]:
        """ì†Œë“/ì§€ì¶œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì‚° ë¶„ë°° ì¶”ì²œ
        
        Args:
            income_data: ì†Œë“ ë°ì´í„°
            expense_data: ì§€ì¶œ ë°ì´í„°
            use_ai: Trueì´ë©´ GPT ì‚¬ìš©, Falseì´ë©´ ê·œì¹™ ê¸°ë°˜ ì‚¬ìš© (ê¸°ë³¸ê°’: False)
        """
        if not income_data or not expense_data:
            return {"message": "ì†Œë“ ë˜ëŠ” ì§€ì¶œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"}

        # ğŸ”¥ ê·œì¹™ ê¸°ë°˜ ì¶”ì²œ (ê¸°ë³¸ê°’)
        if not use_ai:
            from asset_allocation.domain.service.rule_based_allocation_service import RuleBasedAllocationService
            rule_service = RuleBasedAllocationService()
            return rule_service.generate_recommendation(income_data, expense_data, risk_profile="balanced")

        # ğŸ”¥ AI ê¸°ë°˜ ì¶”ì²œ (use_ai=Trueì¼ ë•Œë§Œ)
        # ì•ˆì „í•œ íƒ€ì… ë³€í™˜
        try:
            total_income = int(income_data.get("total_income", 0)) if income_data.get("total_income") else 0
        except (ValueError, TypeError):
            total_income = 0

        try:
            total_expense = int(expense_data.get("total_expense", 0)) if expense_data.get("total_expense") else 0
        except (ValueError, TypeError):
            total_expense = 0

        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ì¬ë¬´ì„¤ê³„ì‚¬ì…ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìì‚° ë¶„ë°°ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì†Œë“ ë¶„ì„:
{json.dumps(income_data, ensure_ascii=False, indent=2)}

ì§€ì¶œ ë¶„ì„:
{json.dumps(expense_data, ensure_ascii=False, indent=2)}

ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. ì¬ë¬´ ê±´ì „ì„± í‰ê°€
   - ì†Œë“ ëŒ€ë¹„ ì§€ì¶œ ë¹„ìœ¨
   - í•„ìˆ˜ì§€ì¶œ ë¹„ìœ¨
   - ì„ íƒì§€ì¶œ ë¹„ìœ¨
   - ì €ì¶•/íˆ¬ì ë¹„ìœ¨

2. ìì‚° ë¶„ë°° ì¶”ì²œ (ì›” ê°€ì²˜ë¶„ì†Œë“ ê¸°ì¤€)
   - ë¹„ìƒìê¸ˆ: Xì› (Y%)
   - ë‹¨ê¸°ì €ì¶•: Xì› (Y%)
   - ì¥ê¸°íˆ¬ì: Xì› (Y%)
   - ë³´í—˜: Xì› (Y%)
   - ê¸°íƒ€: Xì› (Y%)

3. ê°œì„  ì œì•ˆ (ìš°ì„ ìˆœìœ„ ìˆœ)
   - ì¤„ì¼ ìˆ˜ ìˆëŠ” ì§€ì¶œ í•­ëª©
   - ëŠ˜ë ¤ì•¼ í•  í•­ëª©
   - êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ë²•

4. ëª©í‘œë³„ ì €ì¶• ê³„íš
   - ë‹¨ê¸° ëª©í‘œ (1ë…„ ì´ë‚´)
   - ì¤‘ê¸° ëª©í‘œ (1-5ë…„)
   - ì¥ê¸° ëª©í‘œ (5ë…„ ì´ìƒ)

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´:
{{
  "health_score": {{
    "overall": 0-100ì ,
    "income_to_expense_ratio": ë¹„ìœ¨,
    "essential_expense_ratio": ë¹„ìœ¨,
    "savings_ratio": ë¹„ìœ¨,
    "comment": "í‰ê°€ ì½”ë©˜íŠ¸"
  }},
  "asset_allocation": {{
    "emergency_fund": {{"amount": ê¸ˆì•¡, "percentage": ë¹„ìœ¨, "reason": "ì´ìœ "}},
    "short_term_savings": {{"amount": ê¸ˆì•¡, "percentage": ë¹„ìœ¨, "reason": "ì´ìœ "}},
    "long_term_investment": {{"amount": ê¸ˆì•¡, "percentage": ë¹„ìœ¨, "reason": "ì´ìœ "}},
    "insurance": {{"amount": ê¸ˆì•¡, "percentage": ë¹„ìœ¨, "reason": "ì´ìœ "}},
    "other": {{"amount": ê¸ˆì•¡, "percentage": ë¹„ìœ¨, "reason": "ì´ìœ "}}
  }},
  "improvement_suggestions": [
    {{"priority": 1, "category": "ì¹´í…Œê³ ë¦¬", "action": "êµ¬ì²´ì  í–‰ë™", "expected_saving": ì˜ˆìƒì ˆê°ì•¡}},
    {{"priority": 2, "category": "ì¹´í…Œê³ ë¦¬", "action": "êµ¬ì²´ì  í–‰ë™", "expected_saving": ì˜ˆìƒì ˆê°ì•¡}}
  ],
  "savings_goals": {{
    "short_term": {{"target": "ëª©í‘œ", "amount": ê¸ˆì•¡, "months": ê°œì›”}},
    "medium_term": {{"target": "ëª©í‘œ", "amount": ê¸ˆì•¡, "months": ê°œì›”}},
    "long_term": {{"target": "ëª©í‘œ", "amount": ê¸ˆì•¡, "months": ê°œì›”}}
  }}
}}
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2500,
                temperature=0,  # ì¼ê´€ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ë³€ê²½
                seed=12345  # ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥
            )

            result_text = response.choices[0].message.content.strip()
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()

            return json.loads(result_text)
        except Exception as e:
            logger.error(f"[ERROR] Recommendation generation failed: {str(e)}")
            return {"error": str(e)}

    @log_util.logging_decorator
    def _generate_summary(self, income_data: Dict, expense_data: Dict) -> Dict[str, Any]:
        """ì „ì²´ ì¬ë¬´ ìƒí™© ìš”ì•½"""
        # ì•ˆì „í•œ íƒ€ì… ë³€í™˜ - í•œê¸€ í‚¤ ìš°ì„ , ì—†ìœ¼ë©´ ì˜ë¬¸ í‚¤
        try:
            total_income = int(income_data.get("ì´ì†Œë“") or income_data.get("total_income", 0)) if (
                        income_data.get("ì´ì†Œë“") or income_data.get("total_income")) else 0
        except (ValueError, TypeError):
            total_income = 0

        try:
            total_expense = int(expense_data.get("ì´ì§€ì¶œ") or expense_data.get("total_expense", 0)) if (
                        expense_data.get("ì´ì§€ì¶œ") or expense_data.get("total_expense")) else 0
        except (ValueError, TypeError):
            total_expense = 0

        surplus = total_income - total_expense
        surplus_ratio = (surplus / total_income * 100) if total_income > 0 else 0

        return {
            "total_income": total_income,
            "total_expense": total_expense,
            "surplus": surplus,
            "surplus_ratio": round(surplus_ratio, 2),
            "status": "í‘ì" if surplus > 0 else "ì ì" if surplus < 0 else "ìˆ˜ì§€ê· í˜•"
        }
    
    def _learn_from_gpt_income(self, uncertain_items: Dict[str, str], gpt_result: Dict):
        """
        GPTê°€ ë¶„ë¥˜í•œ ì†Œë“ í•­ëª©ì„ DBì— í•™ìŠµ
        
        Args:
            uncertain_items: ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬ ëª»í•œ í•­ëª©ë“¤
            gpt_result: GPT ë¶„ì„ ê²°ê³¼
        """
        try:
            from documents_multi_agents.domain.service.hybrid_parser import HybridParser
            
            learner = HybridParser()
            
            # GPT ê²°ê³¼ì—ì„œ ê° í•­ëª©ì´ ì–´ëŠ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€ í™•ì¸
            for field_name in uncertain_items.keys():
                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ í•´ë‹¹ í•­ëª© ì°¾ê¸°
                found = False
                for category_name, items in gpt_result.items():
                    if category_name in ['ì¹´í…Œê³ ë¦¬ë³„ í•©ê³„', 'ì´ì†Œë“', 'total_income', 'error', 'raw_items']:
                        continue
                    
                    if isinstance(items, dict) and field_name in items:
                        # ì´ í•­ëª©ì€ ì†Œë“ìœ¼ë¡œ ë¶„ë¥˜ë¨
                        learner.learn_from_gpt_result(field_name, 'income')
                        found = True
                        break
                
                if not found:
                    logger.debug(f"[LEARN] í•­ëª© '{field_name}'ì„ GPT ê²°ê³¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        except Exception as e:
            logger.error(f"[LEARN] ì†Œë“ í•™ìŠµ ì˜¤ë¥˜: {str(e)}")
    
    def _learn_from_gpt_expense(self, uncertain_items: Dict[str, str], gpt_result: Dict):
        """
        GPTê°€ ë¶„ë¥˜í•œ ì§€ì¶œ í•­ëª©ì„ DBì— í•™ìŠµ
        
        Args:
            uncertain_items: ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬ ëª»í•œ í•­ëª©ë“¤
            gpt_result: GPT ë¶„ì„ ê²°ê³¼
        """
        try:
            from documents_multi_agents.domain.service.hybrid_parser import HybridParser
            
            learner = HybridParser()
            
            # GPT ê²°ê³¼ì—ì„œ ê° í•­ëª©ì´ ì–´ëŠ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€ í™•ì¸
            for field_name in uncertain_items.keys():
                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ í•´ë‹¹ í•­ëª© ì°¾ê¸°
                found = False
                for category_name, items in gpt_result.items():
                    if category_name in ['ì¹´í…Œê³ ë¦¬ë³„ í•©ê³„', 'ì´ì§€ì¶œ', 'total_expense', 'error', 'raw_items']:
                        continue
                    
                    if isinstance(items, dict) and field_name in items:
                        # ì´ í•­ëª©ì€ ì§€ì¶œë¡œ ë¶„ë¥˜ë¨
                        learner.learn_from_gpt_result(field_name, 'expense')
                        found = True
                        break
                
                if not found:
                    logger.debug(f"[LEARN] í•­ëª© '{field_name}'ì„ GPT ê²°ê³¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        except Exception as e:
            logger.error(f"[LEARN] ì§€ì¶œ í•™ìŠµ ì˜¤ë¥˜: {str(e)}")
